{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import glob as gb\n",
    "import cv2\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering the folder: BrownSpot\n",
      "Number of images in the folder: 1400\n",
      "Entering the folder: Blast\n",
      "Number of images in the folder: 1240\n",
      "Entering the folder: BacterialBlight\n",
      "Number of images in the folder: 1384\n",
      "Entering the folder: Tungro\n",
      "Number of images in the folder: 1108\n",
      "Entering to the folder name: BrownSpot\n",
      "Number of images in the folder is 200\n",
      "Entering to the folder name: Blast\n",
      "Number of images in the folder is 200\n",
      "Entering to the folder name: BacterialBlight\n",
      "Number of images in the folder is 200\n",
      "Entering to the folder name: Tungro\n",
      "Number of images in the folder is 200\n",
      "(5132, 224, 224, 3)\n",
      "********************\n",
      "(5132,)\n",
      "********************\n",
      "(800, 224, 224, 3)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "trainpath = '/Users/amanmaurya/Desktop/Rice Leaf Disease Image Classification/dataset/train'\n",
    "testpath = '/Users/amanmaurya/Desktop/Rice Leaf Disease Image Classification/dataset/test'\n",
    "\n",
    "# Image Processing - Training Data\n",
    "new_size = 224\n",
    "train_images = []\n",
    "train_labels = []\n",
    "class_disease = {'BacterialBlight': 0, 'Blast': 1, 'BrownSpot': 2, 'Tungro': 3}\n",
    "\n",
    "for i in os.listdir(trainpath):\n",
    "    if i in class_disease:\n",
    "        print(\"Entering the folder:\", i)\n",
    "        files = gb.glob(pathname=str(trainpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(trainpath + '/' + i + '/*.JPG'))\n",
    "        print(\"Number of images in the folder:\", len(files))\n",
    "        for j in files:\n",
    "            image_raw = cv2.imread(j)\n",
    "            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n",
    "            resize_image = cv2.resize(image, (new_size, new_size))\n",
    "            train_images.append(list(resize_image))\n",
    "            train_labels.append(class_disease[i])\n",
    "\n",
    "# Image Processing - Testing Data\n",
    "new_size = 224\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for i in os.listdir(testpath):\n",
    "    if i in class_disease:\n",
    "        print(\"Entering to the folder name:\", i)\n",
    "        files = gb.glob(pathname=str(testpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(testpath + '/' + i + '/*.JPG'))\n",
    "        print(\"Number of images in the folder is\", len(files))\n",
    "        for j in files:\n",
    "            image_raw = cv2.imread(j)\n",
    "            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n",
    "            resize_image = cv2.resize(image, (new_size, new_size))\n",
    "            test_images.append(list(resize_image))\n",
    "            test_labels.append(class_disease[i])\n",
    "\n",
    "def list_to_array_train(train_images, train_labels):\n",
    "    return np.array(train_images), np.array(train_labels)\n",
    "\n",
    "X_train, y_train = list_to_array_train(train_images, train_labels)\n",
    "\n",
    "def list_to_array_test(test_images, test_labels):\n",
    "    return np.array(test_images), np.array(test_labels)\n",
    "\n",
    "X_test, y_test = list_to_array_test(test_images, test_labels)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"*\" * 20)\n",
    "print(y_train.shape)\n",
    "print(\"*\" * 20)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "def keras_to_categorical(y_train, y_test):\n",
    "    return to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "y_train1 = y_train\n",
    "y_test1 = y_test\n",
    "y_train, y_test = keras_to_categorical(y_train, y_test)\n",
    "\n",
    "y_train1.shape, y_test1.shape\n",
    "\n",
    "def convert_one_hot_to_categorical(one_hot_labels):\n",
    "    return np.argmax(one_hot_labels, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm  # for progress bar if needed\n",
    "import math\n",
    "\n",
    "def compute_gabor_features(image, frequencies=[0.4, 1.6], angles=[0, np.pi/2], grid_size=(4, 4)):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    gabor_features = []\n",
    "    \n",
    "    for frequency in frequencies:\n",
    "        for angle in angles:\n",
    "            g_kernel = cv2.getGaborKernel((grid_size[0], grid_size[1]), frequency, angle, 5.0, 1.0, 0, ktype=cv2.CV_32F)\n",
    "            filtered_image = cv2.filter2D(gray_image, cv2.CV_8UC3, g_kernel)\n",
    "            hist, _ = np.histogram(filtered_image.ravel(), bins=np.arange(0, 256), range=(0, 256))\n",
    "            hist = hist.astype(\"float\")\n",
    "            hist /= (hist.sum() + 1e-6)  # Normalize the histogram\n",
    "            gabor_features.extend(hist)\n",
    "    \n",
    "    return np.array(gabor_features)\n",
    "\n",
    "def extract_gabor_features(images, frequencies=[0.4, 1.6], angles=[0, np.pi/2], grid_size=(4, 4)):\n",
    "    gabor_features = []\n",
    "    # Initialize tqdm for progress bar if needed\n",
    "    for img in tqdm(images, desc=\"Extracting Gabor features\"):\n",
    "        gabor_feature = compute_gabor_features(img, frequencies=frequencies, angles=angles, grid_size=grid_size)\n",
    "        gabor_features.append(gabor_feature)\n",
    "    \n",
    "    return np.array(gabor_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Gabor features: 100%|██████████| 5132/5132 [00:20<00:00, 251.13it/s]\n",
      "Extracting Gabor features: 100%|██████████| 800/800 [00:03<00:00, 257.90it/s]\n"
     ]
    }
   ],
   "source": [
    "train_feature_gabor = extract_gabor_features(X_train)\n",
    "test_feature_gabor = extract_gabor_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gabor + Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - Accuracy: 0.99375000, Recall: 0.99375000, Precision: 0.99630542, F1 Score: 0.99497440, AUC: 0.99625000\n",
      "Run 2 - Accuracy: 0.99375000, Recall: 0.99375000, Precision: 1.00000000, F1 Score: 0.99683544, AUC: 0.99687500\n",
      "Run 3 - Accuracy: 0.99375000, Recall: 0.99375000, Precision: 0.99875622, F1 Score: 0.99621200, AUC: 0.99666667\n",
      "Run 4 - Accuracy: 0.99375000, Recall: 0.99375000, Precision: 1.00000000, F1 Score: 0.99683544, AUC: 0.99687500\n",
      "Run 5 - Accuracy: 0.99375000, Recall: 0.99375000, Precision: 1.00000000, F1 Score: 0.99683544, AUC: 0.99687500\n",
      "Run 6 - Accuracy: 0.99375000, Recall: 0.99375000, Precision: 1.00000000, F1 Score: 0.99683544, AUC: 0.99687500\n",
      "Run 7 - Accuracy: 0.99375000, Recall: 0.99375000, Precision: 0.99752475, F1 Score: 0.99559166, AUC: 0.99645833\n",
      "Run 8 - Accuracy: 0.99375000, Recall: 0.99375000, Precision: 0.99630542, F1 Score: 0.99497440, AUC: 0.99625000\n",
      "Run 9 - Accuracy: 0.99375000, Recall: 0.99375000, Precision: 1.00000000, F1 Score: 0.99683544, AUC: 0.99687500\n",
      "Run 10 - Accuracy: 0.99625000, Recall: 0.99625000, Precision: 1.00000000, F1 Score: 0.99811083, AUC: 0.99812500\n",
      "\n",
      "Average Metrics:\n",
      "Accuracy: 0.99400000 (std: 0.00075000)\n",
      "Recall: 0.99400000 (std: 0.00075000)\n",
      "Precision: 0.99888918 (std: 0.00150448)\n",
      "F1: 0.99640405 (std: 0.00092888)\n",
      "Auc: 0.99681250 (std: 0.00050217)\n",
      "\n",
      "Metrics results saved to 'gabor+RF.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_fuse_and_evaluate_model(train_feature_gabor, y_train, test_feature_gabor, y_test):\n",
    "\n",
    "        rf = RandomForestClassifier()\n",
    "        rf = rf.fit(train_feature_gabor, y_train)\n",
    "        test_pred = rf.predict(test_feature_gabor)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, test_pred)\n",
    "        recall = recall_score(y_test, test_pred, average='weighted')\n",
    "        precision = precision_score(y_test, test_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n",
    "\n",
    "        return accuracy, recall, precision, f1, auc\n",
    "\n",
    "num_runs = 10\n",
    "results = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n",
    "\n",
    "for i in range(num_runs):\n",
    "    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_gabor, y_train, test_feature_gabor, y_test)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['precision'].append(precision)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(auc)\n",
    "    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "average_metrics = {metric: np.mean(values) for metric, values in results.items()}\n",
    "std_metrics = {metric: np.std(values) for metric, values in results.items()}\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'gabor+RF.xlsx'\n",
    "results_df.to_excel(output_file, index_label='Run')\n",
    "\n",
    "print(f\"\\nMetrics results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gabor + Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - Accuracy: 0.95875000, Recall: 0.95875000, Precision: 0.95890626, F1 Score: 0.95875916, AUC: 0.97250000\n",
      "Run 2 - Accuracy: 0.94750000, Recall: 0.94750000, Precision: 0.94768704, F1 Score: 0.94755443, AUC: 0.96500000\n",
      "Run 3 - Accuracy: 0.95500000, Recall: 0.95500000, Precision: 0.95498062, F1 Score: 0.95496078, AUC: 0.97000000\n",
      "Run 4 - Accuracy: 0.94875000, Recall: 0.94875000, Precision: 0.94887482, F1 Score: 0.94873676, AUC: 0.96583333\n",
      "Run 5 - Accuracy: 0.94875000, Recall: 0.94875000, Precision: 0.94898808, F1 Score: 0.94865704, AUC: 0.96583333\n",
      "Run 6 - Accuracy: 0.95500000, Recall: 0.95500000, Precision: 0.95518091, F1 Score: 0.95494556, AUC: 0.97000000\n",
      "Run 7 - Accuracy: 0.96500000, Recall: 0.96500000, Precision: 0.96498587, F1 Score: 0.96498394, AUC: 0.97666667\n",
      "Run 8 - Accuracy: 0.95000000, Recall: 0.95000000, Precision: 0.95086115, F1 Score: 0.95002023, AUC: 0.96666667\n",
      "Run 9 - Accuracy: 0.94875000, Recall: 0.94875000, Precision: 0.94867534, F1 Score: 0.94867997, AUC: 0.96583333\n",
      "Run 10 - Accuracy: 0.95125000, Recall: 0.95125000, Precision: 0.95142121, F1 Score: 0.95126798, AUC: 0.96750000\n",
      "\n",
      "Average Metrics:\n",
      "Accuracy: 0.95287500 (std: 0.00530477)\n",
      "Recall: 0.95287500 (std: 0.00530477)\n",
      "Precision: 0.95305613 (std: 0.00523739)\n",
      "F1: 0.95285659 (std: 0.00530509)\n",
      "Auc: 0.96858333 (std: 0.00353652)\n",
      "\n",
      "Metrics results saved to 'gabor+DTC.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_fuse_and_evaluate_model(train_feature_gabor, y_train, test_feature_gabor, y_test):\n",
    "\n",
    "        dt = DecisionTreeClassifier()\n",
    "        dt.fit(train_feature_gabor, y_train)\n",
    "        test_pred = dt.predict(test_feature_gabor)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, test_pred)\n",
    "        recall = recall_score(y_test, test_pred, average='weighted')\n",
    "        precision = precision_score(y_test, test_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n",
    "\n",
    "        return accuracy, recall, precision, f1, auc\n",
    "\n",
    "num_runs = 10\n",
    "results = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n",
    "\n",
    "for i in range(num_runs):\n",
    "    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_gabor, y_train, test_feature_gabor, y_test)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['precision'].append(precision)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(auc)\n",
    "    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "average_metrics = {metric: np.mean(values) for metric, values in results.items()}\n",
    "std_metrics = {metric: np.std(values) for metric, values in results.items()}\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'gabor+DTC.xlsx'\n",
    "results_df.to_excel(output_file, index_label='Run')\n",
    "\n",
    "print(f\"\\nMetrics results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gabor + KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "Run 2 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "Run 3 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "Run 4 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "Run 5 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "Run 6 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "Run 7 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "Run 8 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "Run 9 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "Run 10 - Accuracy: 0.98000000, Recall: 0.98000000, Precision: 0.98148148, F1 Score: 0.97996795, AUC: 0.98666667\n",
      "\n",
      "Average Metrics:\n",
      "Accuracy: 0.98000000 (std: 0.00000000)\n",
      "Recall: 0.98000000 (std: 0.00000000)\n",
      "Precision: 0.98148148 (std: 0.00000000)\n",
      "F1: 0.97996795 (std: 0.00000000)\n",
      "Auc: 0.98666667 (std: 0.00000000)\n",
      "\n",
      "Metrics results saved to 'gabor+KNN.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train_fuse_and_evaluate_model(train_feature_gabor, y_train, test_feature_gabor, y_test):\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(train_feature_gabor, y_train)\n",
    "        test_pred = knn.predict(test_feature_gabor)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, test_pred)\n",
    "        recall = recall_score(y_test, test_pred, average='weighted')\n",
    "        precision = precision_score(y_test, test_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n",
    "\n",
    "        return accuracy, recall, precision, f1, auc\n",
    "\n",
    "num_runs = 10\n",
    "results = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n",
    "\n",
    "for i in range(num_runs):\n",
    "    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_gabor, y_train, test_feature_gabor, y_test)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['precision'].append(precision)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(auc)\n",
    "    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "average_metrics = {metric: np.mean(values) for metric, values in results.items()}\n",
    "std_metrics = {metric: np.std(values) for metric, values in results.items()}\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'gabor+KNN.xlsx'\n",
    "results_df.to_excel(output_file, index_label='Run')\n",
    "\n",
    "print(f\"\\nMetrics results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gabor + SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - Accuracy: 0.58750000, Recall: 0.58750000, Precision: 0.65606673, F1 Score: 0.54055663, AUC: 0.86541458\n",
      "Run 2 - Accuracy: 0.58750000, Recall: 0.58750000, Precision: 0.65606673, F1 Score: 0.54055663, AUC: 0.86560000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def train_fuse_and_evaluate_model(train_feature_gabor, y_train, test_feature_gabor, y_test):\n",
    "\n",
    "        y_train_cat = convert_one_hot_to_categorical(y_train)\n",
    "        y_test_cat = convert_one_hot_to_categorical(y_test)\n",
    "\n",
    "        # Define and train SVM Classifier\n",
    "        svm = SVC(probability=True)\n",
    "        svm.fit(train_feature_gabor, y_train_cat)\n",
    "        test_pred = svm.predict(test_feature_gabor)\n",
    "        test_pred_proba = svm.predict_proba(test_feature_gabor)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test_cat, test_pred)\n",
    "        recall = recall_score(y_test_cat, test_pred, average='weighted')\n",
    "        precision = precision_score(y_test_cat, test_pred, average='weighted')\n",
    "        f1 = f1_score(y_test_cat, test_pred, average='weighted')\n",
    "        auc = roc_auc_score(y_test_cat, test_pred_proba, multi_class='ovr', average='weighted')\n",
    "\n",
    "        return accuracy, recall, precision, f1, auc\n",
    "\n",
    "num_runs = 10\n",
    "results = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n",
    "\n",
    "for i in range(num_runs):\n",
    "    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_gabor, y_train, test_feature_gabor, y_test)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['precision'].append(precision)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(auc)\n",
    "    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "average_metrics = {metric: np.mean(values) for metric, values in results.items()}\n",
    "std_metrics = {metric: np.std(values) for metric, values in results.items()}\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'gabor+SVM.xlsx'\n",
    "results_df.to_excel(output_file, index_label='Run')\n",
    "\n",
    "print(f\"\\nMetrics results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
