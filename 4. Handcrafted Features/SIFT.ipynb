{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8835279,"sourceType":"datasetVersion","datasetId":5316766}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pathlib\nimport os\nimport glob as gb\nimport cv2\nimport PIL\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom tensorflow.keras.utils import to_categorical\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:26:20.559008Z","iopub.execute_input":"2024-07-10T08:26:20.559469Z","iopub.status.idle":"2024-07-10T08:26:41.792363Z","shell.execute_reply.started":"2024-07-10T08:26:20.559430Z","shell.execute_reply":"2024-07-10T08:26:41.791019Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-10 08:26:27.476769: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-10 08:26:27.476933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-10 08:26:27.708777: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define paths\ntrainpath = '/kaggle/input/riceleafdiseasedataset/dataset/train'\ntestpath = '/kaggle/input/riceleafdiseasedataset/dataset/test'\n\n# Image Processing - Training Data\nnew_size = 224\ntrain_images = []\ntrain_labels = []\nclass_disease = {'BacterialBlight': 0, 'Blast': 1, 'BrownSpot': 2, 'Tungro': 3}\n\nfor i in os.listdir(trainpath):\n    if i in class_disease:\n        print(\"Entering the folder:\", i)\n        files = gb.glob(pathname=str(trainpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(trainpath + '/' + i + '/*.JPG'))\n        print(\"Number of images in the folder:\", len(files))\n        for j in files:\n            image_raw = cv2.imread(j)\n            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n            resize_image = cv2.resize(image, (new_size, new_size))\n            train_images.append(list(resize_image))\n            train_labels.append(class_disease[i])\n\n# Image Processing - Testing Data\nnew_size = 224\ntest_images = []\ntest_labels = []\n\nfor i in os.listdir(testpath):\n    if i in class_disease:\n        print(\"Entering to the folder name:\", i)\n        files = gb.glob(pathname=str(testpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(testpath + '/' + i + '/*.JPG'))\n        print(\"Number of images in the folder is\", len(files))\n        for j in files:\n            image_raw = cv2.imread(j)\n            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n            resize_image = cv2.resize(image, (new_size, new_size))\n            test_images.append(list(resize_image))\n            test_labels.append(class_disease[i])\n\ndef list_to_array_train(train_images, train_labels):\n    return np.array(train_images), np.array(train_labels)\n\nX_train, y_train = list_to_array_train(train_images, train_labels)\n\ndef list_to_array_test(test_images, test_labels):\n    return np.array(test_images), np.array(test_labels)\n\nX_test, y_test = list_to_array_test(test_images, test_labels)\n\nprint(X_train.shape)\nprint(\"*\" * 20)\nprint(y_train.shape)\nprint(\"*\" * 20)\nprint(X_test.shape)\nprint(y_test.shape)\n\ndef keras_to_categorical(y_train, y_test):\n    return to_categorical(y_train), to_categorical(y_test)\n\ny_train1 = y_train\ny_test1 = y_test\ny_train, y_test = keras_to_categorical(y_train, y_test)\n\ny_train1.shape, y_test1.shape\n\ndef convert_one_hot_to_categorical(one_hot_labels):\n    return np.argmax(one_hot_labels, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:26:41.795046Z","iopub.execute_input":"2024-07-10T08:26:41.795973Z","iopub.status.idle":"2024-07-10T08:27:19.717178Z","shell.execute_reply.started":"2024-07-10T08:26:41.795925Z","shell.execute_reply":"2024-07-10T08:27:19.715912Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Entering the folder: Tungro\nNumber of images in the folder: 1108\nEntering the folder: BacterialBlight\nNumber of images in the folder: 1384\nEntering the folder: Blast\nNumber of images in the folder: 1240\nEntering the folder: BrownSpot\nNumber of images in the folder: 1400\nEntering to the folder name: Tungro\nNumber of images in the folder is 200\nEntering to the folder name: BacterialBlight\nNumber of images in the folder is 200\nEntering to the folder name: Blast\nNumber of images in the folder is 200\nEntering to the folder name: BrownSpot\nNumber of images in the folder is 200\n(5132, 224, 224, 3)\n********************\n(5132,)\n********************\n(800, 224, 224, 3)\n(800,)\n","output_type":"stream"}]},{"cell_type":"code","source":"def extract_sift_features(images, max_features=128):\n    sift = cv2.SIFT_create()\n    sift_features = []\n    for img in images:\n        if len(img.shape) == 3:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        keypoints, descriptors = sift.detectAndCompute(img, None)\n        if descriptors is not None:\n            descriptors = descriptors.flatten()\n            if len(descriptors) < max_features:\n                descriptors = np.pad(descriptors, (0, max_features - len(descriptors)), 'constant')\n            else:\n                descriptors = descriptors[:max_features]\n        else:\n            descriptors = np.zeros(max_features)\n        sift_features.append(descriptors)\n    return np.array(sift_features)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:27:19.718720Z","iopub.execute_input":"2024-07-10T08:27:19.719121Z","iopub.status.idle":"2024-07-10T08:27:19.728565Z","shell.execute_reply.started":"2024-07-10T08:27:19.719075Z","shell.execute_reply":"2024-07-10T08:27:19.727136Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_feature_sift = extract_sift_features(X_train)\ntest_feature_sift = extract_sift_features(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:27:19.731459Z","iopub.execute_input":"2024-07-10T08:27:19.731878Z","iopub.status.idle":"2024-07-10T08:29:01.015567Z","shell.execute_reply.started":"2024-07-10T08:27:19.731844Z","shell.execute_reply":"2024-07-10T08:29:01.014131Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# sift + Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test):\n\n        rf = RandomForestClassifier()\n        rf = rf.fit(train_feature_sift, y_train)\n        test_pred = rf.predict(test_feature_sift)\n\n        accuracy = accuracy_score(y_test, test_pred)\n        recall = recall_score(y_test, test_pred, average='weighted')\n        precision = precision_score(y_test, test_pred, average='weighted')\n        f1 = f1_score(y_test, test_pred, average='weighted')\n        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'sift+RF.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:29:01.017389Z","iopub.execute_input":"2024-07-10T08:29:01.017759Z","iopub.status.idle":"2024-07-10T08:29:48.102579Z","shell.execute_reply.started":"2024-07-10T08:29:01.017727Z","shell.execute_reply":"2024-07-10T08:29:48.101363Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.38750000, Recall: 0.38750000, Precision: 0.98622788, F1 Score: 0.54647303, AUC: 0.69291667\nRun 2 - Accuracy: 0.41000000, Recall: 0.41000000, Precision: 0.98012452, F1 Score: 0.57278341, AUC: 0.70375000\nRun 3 - Accuracy: 0.40125000, Recall: 0.40125000, Precision: 0.97523920, F1 Score: 0.56389430, AUC: 0.69895833\nRun 4 - Accuracy: 0.39625000, Recall: 0.39625000, Precision: 0.97083424, F1 Score: 0.55354101, AUC: 0.69625000\nRun 5 - Accuracy: 0.38750000, Recall: 0.38750000, Precision: 0.99009901, F1 Score: 0.55094336, AUC: 0.69291667\nRun 6 - Accuracy: 0.41250000, Recall: 0.41250000, Precision: 0.99336283, F1 Score: 0.57231683, AUC: 0.70562500\nRun 7 - Accuracy: 0.40250000, Recall: 0.40250000, Precision: 0.98781364, F1 Score: 0.56225441, AUC: 0.70020833\nRun 8 - Accuracy: 0.39375000, Recall: 0.39375000, Precision: 0.98970640, F1 Score: 0.55653594, AUC: 0.69604167\nRun 9 - Accuracy: 0.40625000, Recall: 0.40625000, Precision: 0.98761615, F1 Score: 0.56825057, AUC: 0.70229167\nRun 10 - Accuracy: 0.39625000, Recall: 0.39625000, Precision: 0.99278846, F1 Score: 0.55902487, AUC: 0.69750000\n\nAverage Metrics:\nAccuracy: 0.39937500 (std: 0.00823958)\nRecall: 0.39937500 (std: 0.00823958)\nPrecision: 0.98538123 (std: 0.00716432)\nF1: 0.56060177 (std: 0.00848227)\nAuc: 0.69864583 (std: 0.00412400)\n\nMetrics results saved to 'sift+RF.xlsx'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# sift + Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test):\n\n        dt = DecisionTreeClassifier()\n        dt.fit(train_feature_sift, y_train)\n        test_pred = dt.predict(test_feature_sift)\n\n        accuracy = accuracy_score(y_test, test_pred)\n        recall = recall_score(y_test, test_pred, average='weighted')\n        precision = precision_score(y_test, test_pred, average='weighted')\n        f1 = f1_score(y_test, test_pred, average='weighted')\n        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'sift+DTC.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:29:48.104079Z","iopub.execute_input":"2024-07-10T08:29:48.104783Z","iopub.status.idle":"2024-07-10T08:29:54.869152Z","shell.execute_reply.started":"2024-07-10T08:29:48.104748Z","shell.execute_reply":"2024-07-10T08:29:54.867178Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.56125000, Recall: 0.56125000, Precision: 0.56131058, F1 Score: 0.56013359, AUC: 0.70750000\nRun 2 - Accuracy: 0.55000000, Recall: 0.55000000, Precision: 0.55044278, F1 Score: 0.54928249, AUC: 0.70000000\nRun 3 - Accuracy: 0.56625000, Recall: 0.56625000, Precision: 0.56638251, F1 Score: 0.56560877, AUC: 0.71083333\nRun 4 - Accuracy: 0.54875000, Recall: 0.54875000, Precision: 0.55120321, F1 Score: 0.54836672, AUC: 0.69916667\nRun 5 - Accuracy: 0.56375000, Recall: 0.56375000, Precision: 0.56306239, F1 Score: 0.56242840, AUC: 0.70916667\nRun 6 - Accuracy: 0.56625000, Recall: 0.56625000, Precision: 0.56592556, F1 Score: 0.56569422, AUC: 0.71083333\nRun 7 - Accuracy: 0.54750000, Recall: 0.54750000, Precision: 0.54783824, F1 Score: 0.54644590, AUC: 0.69833333\nRun 8 - Accuracy: 0.55000000, Recall: 0.55000000, Precision: 0.55050838, F1 Score: 0.54943599, AUC: 0.70000000\nRun 9 - Accuracy: 0.57000000, Recall: 0.57000000, Precision: 0.57017653, F1 Score: 0.56900610, AUC: 0.71333333\nRun 10 - Accuracy: 0.56500000, Recall: 0.56500000, Precision: 0.56507172, F1 Score: 0.56470802, AUC: 0.71000000\n\nAverage Metrics:\nAccuracy: 0.55887500 (std: 0.00830004)\nRecall: 0.55887500 (std: 0.00830004)\nPrecision: 0.55919219 (std: 0.00785128)\nF1: 0.55811102 (std: 0.00826378)\nAuc: 0.70591667 (std: 0.00553336)\n\nMetrics results saved to 'sift+DTC.xlsx'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# sift + KNN Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test):\n\n        knn = KNeighborsClassifier(n_neighbors=5)\n        knn.fit(train_feature_sift, y_train)\n        test_pred = knn.predict(test_feature_sift)\n\n        accuracy = accuracy_score(y_test, test_pred)\n        recall = recall_score(y_test, test_pred, average='weighted')\n        precision = precision_score(y_test, test_pred, average='weighted')\n        f1 = f1_score(y_test, test_pred, average='weighted')\n        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'sift+KNN.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:29:54.870588Z","iopub.execute_input":"2024-07-10T08:29:54.870960Z","iopub.status.idle":"2024-07-10T08:29:58.525474Z","shell.execute_reply.started":"2024-07-10T08:29:54.870927Z","shell.execute_reply":"2024-07-10T08:29:58.524258Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\nRun 2 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\nRun 3 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\nRun 4 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\nRun 5 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\nRun 6 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\nRun 7 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\nRun 8 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\nRun 9 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\nRun 10 - Accuracy: 0.41125000, Recall: 0.41125000, Precision: 0.54783309, F1 Score: 0.46153883, AUC: 0.64500000\n\nAverage Metrics:\nAccuracy: 0.41125000 (std: 0.00000000)\nRecall: 0.41125000 (std: 0.00000000)\nPrecision: 0.54783309 (std: 0.00000000)\nF1: 0.46153883 (std: 0.00000000)\nAuc: 0.64500000 (std: 0.00000000)\n\nMetrics results saved to 'sift+KNN.xlsx'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# sift + SVM Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\ndef train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test):\n\n        y_train_cat = convert_one_hot_to_categorical(y_train)\n        y_test_cat = convert_one_hot_to_categorical(y_test)\n\n        # Define and train SVM Classifier\n        svm = SVC(probability=True)\n        svm.fit(train_feature_sift, y_train_cat)\n        test_pred = svm.predict(test_feature_sift)\n        test_pred_proba = svm.predict_proba(test_feature_sift)\n\n        # Calculate evaluation metrics\n        accuracy = accuracy_score(y_test_cat, test_pred)\n        recall = recall_score(y_test_cat, test_pred, average='weighted')\n        precision = precision_score(y_test_cat, test_pred, average='weighted')\n        f1 = f1_score(y_test_cat, test_pred, average='weighted')\n        auc = roc_auc_score(y_test_cat, test_pred_proba, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'sift+SVM.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:29:58.527026Z","iopub.execute_input":"2024-07-10T08:29:58.527451Z","iopub.status.idle":"2024-07-10T08:32:50.426667Z","shell.execute_reply.started":"2024-07-10T08:29:58.527413Z","shell.execute_reply":"2024-07-10T08:32:50.425377Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77217917\nRun 2 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77181667\nRun 3 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77220000\nRun 4 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77216250\nRun 5 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77225000\nRun 6 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77216875\nRun 7 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77227083\nRun 8 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77213542\nRun 9 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77209375\nRun 10 - Accuracy: 0.51125000, Recall: 0.51125000, Precision: 0.52006392, F1 Score: 0.50708452, AUC: 0.77207083\n\nAverage Metrics:\nAccuracy: 0.51125000 (std: 0.00000000)\nRecall: 0.51125000 (std: 0.00000000)\nPrecision: 0.52006392 (std: 0.00000000)\nF1: 0.50708452 (std: 0.00000000)\nAuc: 0.77213479 (std: 0.00012120)\n\nMetrics results saved to 'sift+SVM.xlsx'\n","output_type":"stream"}]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test):\n\n        y_train_cat = convert_one_hot_to_categorical(y_train)\n        y_test_cat = convert_one_hot_to_categorical(y_test)\n\n        xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n        xgb.fit(train_feature_sift, y_train_cat)\n        test_pred = xgb.predict(test_feature_sift)\n        test_pred_proba = xgb.predict_proba(test_feature_sift)\n\n        # Calculate evaluation metrics\n        accuracy = accuracy_score(y_test_cat, test_pred)\n        recall = recall_score(y_test_cat, test_pred, average='weighted')\n        precision = precision_score(y_test_cat, test_pred, average='weighted')\n        f1 = f1_score(y_test_cat, test_pred, average='weighted')\n        auc = roc_auc_score(y_test_cat, test_pred_proba, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_sift, y_train, test_feature_sift, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'sift+XGB.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:35:09.597852Z","iopub.execute_input":"2024-07-10T08:35:09.598377Z","iopub.status.idle":"2024-07-10T08:35:53.931513Z","shell.execute_reply.started":"2024-07-10T08:35:09.598336Z","shell.execute_reply":"2024-07-10T08:35:53.930246Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\nRun 2 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\nRun 3 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\nRun 4 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\nRun 5 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\nRun 6 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\nRun 7 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\nRun 8 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\nRun 9 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\nRun 10 - Accuracy: 0.64750000, Recall: 0.64750000, Precision: 0.65078422, F1 Score: 0.64650950, AUC: 0.86420417\n\nAverage Metrics:\nAccuracy: 0.64750000 (std: 0.00000000)\nRecall: 0.64750000 (std: 0.00000000)\nPrecision: 0.65078422 (std: 0.00000000)\nF1: 0.64650950 (std: 0.00000000)\nAuc: 0.86420417 (std: 0.00000000)\n\nMetrics results saved to 'sift+XGB.xlsx'\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}