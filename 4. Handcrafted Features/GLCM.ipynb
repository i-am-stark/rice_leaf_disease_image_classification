{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8835279,"sourceType":"datasetVersion","datasetId":5316766}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pathlib\nimport os\nimport glob as gb\nimport cv2\nimport PIL\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom tensorflow.keras.utils import to_categorical\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:42:39.229803Z","iopub.execute_input":"2024-07-10T08:42:39.230280Z","iopub.status.idle":"2024-07-10T08:42:39.238123Z","shell.execute_reply.started":"2024-07-10T08:42:39.230244Z","shell.execute_reply":"2024-07-10T08:42:39.236555Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define paths\ntrainpath = '/kaggle/input/riceleafdiseasedataset/dataset/train'\ntestpath = '/kaggle/input/riceleafdiseasedataset/dataset/test'\n\n# Image Processing - Training Data\nnew_size = 224\ntrain_images = []\ntrain_labels = []\nclass_disease = {'BacterialBlight': 0, 'Blast': 1, 'BrownSpot': 2, 'Tungro': 3}\n\nfor i in os.listdir(trainpath):\n    if i in class_disease:\n        print(\"Entering the folder:\", i)\n        files = gb.glob(pathname=str(trainpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(trainpath + '/' + i + '/*.JPG'))\n        print(\"Number of images in the folder:\", len(files))\n        for j in files:\n            image_raw = cv2.imread(j)\n            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n            resize_image = cv2.resize(image, (new_size, new_size))\n            train_images.append(list(resize_image))\n            train_labels.append(class_disease[i])\n\n# Image Processing - Testing Data\nnew_size = 224\ntest_images = []\ntest_labels = []\n\nfor i in os.listdir(testpath):\n    if i in class_disease:\n        print(\"Entering to the folder name:\", i)\n        files = gb.glob(pathname=str(testpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(testpath + '/' + i + '/*.JPG'))\n        print(\"Number of images in the folder is\", len(files))\n        for j in files:\n            image_raw = cv2.imread(j)\n            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n            resize_image = cv2.resize(image, (new_size, new_size))\n            test_images.append(list(resize_image))\n            test_labels.append(class_disease[i])\n\ndef list_to_array_train(train_images, train_labels):\n    return np.array(train_images), np.array(train_labels)\n\nX_train, y_train = list_to_array_train(train_images, train_labels)\n\ndef list_to_array_test(test_images, test_labels):\n    return np.array(test_images), np.array(test_labels)\n\nX_test, y_test = list_to_array_test(test_images, test_labels)\n\nprint(X_train.shape)\nprint(\"*\" * 20)\nprint(y_train.shape)\nprint(\"*\" * 20)\nprint(X_test.shape)\nprint(y_test.shape)\n\ndef keras_to_categorical(y_train, y_test):\n    return to_categorical(y_train), to_categorical(y_test)\n\ny_train1 = y_train\ny_test1 = y_test\ny_train, y_test = keras_to_categorical(y_train, y_test)\n\ny_train1.shape, y_test1.shape\n\ndef convert_one_hot_to_categorical(one_hot_labels):\n    return np.argmax(one_hot_labels, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:42:39.240713Z","iopub.execute_input":"2024-07-10T08:42:39.241120Z","iopub.status.idle":"2024-07-10T08:42:55.088609Z","shell.execute_reply.started":"2024-07-10T08:42:39.241087Z","shell.execute_reply":"2024-07-10T08:42:55.087368Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Entering the folder: Tungro\nNumber of images in the folder: 1108\nEntering the folder: BacterialBlight\nNumber of images in the folder: 1384\nEntering the folder: Blast\nNumber of images in the folder: 1240\nEntering the folder: BrownSpot\nNumber of images in the folder: 1400\nEntering to the folder name: Tungro\nNumber of images in the folder is 200\nEntering to the folder name: BacterialBlight\nNumber of images in the folder is 200\nEntering to the folder name: Blast\nNumber of images in the folder is 200\nEntering to the folder name: BrownSpot\nNumber of images in the folder is 200\n(5132, 224, 224, 3)\n********************\n(5132,)\n********************\n(800, 224, 224, 3)\n(800,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom skimage.feature import graycomatrix, graycoprops\nfrom skimage.color import rgb2gray\nfrom tqdm import tqdm  # for progress bar if needed\n\ndef compute_glcm_features(image, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256):\n    if image.ndim == 3:  # Convert RGB to grayscale if necessary\n        gray_image = rgb2gray(image)\n    else:\n        gray_image = image.astype(np.uint8)  # Convert to uint8 if not already\n    \n    # Ensure image is in uint8 format\n    if gray_image.dtype != np.uint8:\n        gray_image = (gray_image * 255).astype(np.uint8)\n    \n    glcm = graycomatrix(gray_image, distances=distances, angles=angles, levels=levels,\n                        symmetric=True, normed=True)\n    \n    # Calculate GLCM properties\n    contrast = graycoprops(glcm, 'contrast').flatten()\n    dissimilarity = graycoprops(glcm, 'dissimilarity').flatten()\n    homogeneity = graycoprops(glcm, 'homogeneity').flatten()\n    energy = graycoprops(glcm, 'energy').flatten()\n    correlation = graycoprops(glcm, 'correlation').flatten()\n    \n    # Concatenate all GLCM features into one array\n    glcm_features = np.concatenate((contrast, dissimilarity, homogeneity, energy, correlation))\n    \n    return glcm_features\n\ndef extract_glcm_features(images):\n    glcm_features = []\n    # Initialize tqdm for progress bar if needed\n    for img in tqdm(images, desc=\"Extracting GLCM features\"):\n        glcm_feature = compute_glcm_features(img)\n        glcm_features.append(glcm_feature)\n    \n    return np.array(glcm_features)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:48:49.186860Z","iopub.execute_input":"2024-07-10T08:48:49.187957Z","iopub.status.idle":"2024-07-10T08:48:49.200628Z","shell.execute_reply.started":"2024-07-10T08:48:49.187913Z","shell.execute_reply":"2024-07-10T08:48:49.199327Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_feature_glcm = extract_glcm_features(X_train)\ntest_feature_glcm = extract_glcm_features(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:48:50.434200Z","iopub.execute_input":"2024-07-10T08:48:50.434968Z","iopub.status.idle":"2024-07-10T08:53:01.447734Z","shell.execute_reply.started":"2024-07-10T08:48:50.434927Z","shell.execute_reply":"2024-07-10T08:53:01.446524Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Extracting GLCM features: 100%|██████████| 5132/5132 [03:37<00:00, 23.65it/s]\nExtracting GLCM features: 100%|██████████| 800/800 [00:33<00:00, 23.55it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# glcm + Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test):\n\n        rf = RandomForestClassifier()\n        rf = rf.fit(train_feature_glcm, y_train)\n        test_pred = rf.predict(test_feature_glcm)\n\n        accuracy = accuracy_score(y_test, test_pred)\n        recall = recall_score(y_test, test_pred, average='weighted')\n        precision = precision_score(y_test, test_pred, average='weighted')\n        f1 = f1_score(y_test, test_pred, average='weighted')\n        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'glcm+RF.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:53:01.450052Z","iopub.execute_input":"2024-07-10T08:53:01.450462Z","iopub.status.idle":"2024-07-10T08:53:26.351865Z","shell.execute_reply.started":"2024-07-10T08:53:01.450428Z","shell.execute_reply":"2024-07-10T08:53:26.350627Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.99250000, Recall: 0.99250000, Precision: 1.00000000, F1 Score: 0.99619289, AUC: 0.99625000\nRun 2 - Accuracy: 0.99125000, Recall: 0.99125000, Precision: 1.00000000, F1 Score: 0.99556633, AUC: 0.99562500\nRun 3 - Accuracy: 0.98750000, Recall: 0.98750000, Precision: 1.00000000, F1 Score: 0.99366764, AUC: 0.99375000\nRun 4 - Accuracy: 0.99250000, Recall: 0.99250000, Precision: 1.00000000, F1 Score: 0.99619289, AUC: 0.99625000\nRun 5 - Accuracy: 0.98750000, Recall: 0.98750000, Precision: 1.00000000, F1 Score: 0.99366764, AUC: 0.99375000\nRun 6 - Accuracy: 0.99125000, Recall: 0.99125000, Precision: 1.00000000, F1 Score: 0.99556633, AUC: 0.99562500\nRun 7 - Accuracy: 0.99000000, Recall: 0.99000000, Precision: 1.00000000, F1 Score: 0.99493661, AUC: 0.99500000\nRun 8 - Accuracy: 0.98750000, Recall: 0.98750000, Precision: 1.00000000, F1 Score: 0.99366764, AUC: 0.99375000\nRun 9 - Accuracy: 0.99000000, Recall: 0.99000000, Precision: 1.00000000, F1 Score: 0.99493661, AUC: 0.99500000\nRun 10 - Accuracy: 0.99125000, Recall: 0.99125000, Precision: 1.00000000, F1 Score: 0.99556633, AUC: 0.99562500\n\nAverage Metrics:\nAccuracy: 0.99012500 (std: 0.00189159)\nRecall: 0.99012500 (std: 0.00189159)\nPrecision: 1.00000000 (std: 0.00000000)\nF1: 0.99499609 (std: 0.00095612)\nAuc: 0.99506250 (std: 0.00094580)\n\nMetrics results saved to 'glcm+RF.xlsx'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# glcm + Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test):\n\n        dt = DecisionTreeClassifier()\n        dt.fit(train_feature_glcm, y_train)\n        test_pred = dt.predict(test_feature_glcm)\n\n        accuracy = accuracy_score(y_test, test_pred)\n        recall = recall_score(y_test, test_pred, average='weighted')\n        precision = precision_score(y_test, test_pred, average='weighted')\n        f1 = f1_score(y_test, test_pred, average='weighted')\n        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'glcm+DTC.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:53:26.353423Z","iopub.execute_input":"2024-07-10T08:53:26.353850Z","iopub.status.idle":"2024-07-10T08:53:27.965933Z","shell.execute_reply.started":"2024-07-10T08:53:26.353815Z","shell.execute_reply":"2024-07-10T08:53:27.964781Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.98250000, Recall: 0.98250000, Precision: 0.98260989, F1 Score: 0.98250860, AUC: 0.98833333\nRun 2 - Accuracy: 0.98250000, Recall: 0.98250000, Precision: 0.98271899, F1 Score: 0.98251361, AUC: 0.98833333\nRun 3 - Accuracy: 0.97250000, Recall: 0.97250000, Precision: 0.97330716, F1 Score: 0.97248008, AUC: 0.98166667\nRun 4 - Accuracy: 0.97625000, Recall: 0.97625000, Precision: 0.97668526, F1 Score: 0.97623409, AUC: 0.98416667\nRun 5 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.97789024, F1 Score: 0.97749195, AUC: 0.98500000\nRun 6 - Accuracy: 0.98125000, Recall: 0.98125000, Precision: 0.98148666, F1 Score: 0.98126382, AUC: 0.98750000\nRun 7 - Accuracy: 0.97500000, Recall: 0.97500000, Precision: 0.97539237, F1 Score: 0.97501052, AUC: 0.98333333\nRun 8 - Accuracy: 0.96625000, Recall: 0.96625000, Precision: 0.96788891, F1 Score: 0.96625726, AUC: 0.97750000\nRun 9 - Accuracy: 0.98500000, Recall: 0.98500000, Precision: 0.98528046, F1 Score: 0.98500736, AUC: 0.99000000\nRun 10 - Accuracy: 0.97625000, Recall: 0.97625000, Precision: 0.97682844, F1 Score: 0.97625748, AUC: 0.98416667\n\nAverage Metrics:\nAccuracy: 0.97750000 (std: 0.00530330)\nRecall: 0.97750000 (std: 0.00530330)\nPrecision: 0.97800884 (std: 0.00492799)\nF1: 0.97750248 (std: 0.00530747)\nAuc: 0.98500000 (std: 0.00353553)\n\nMetrics results saved to 'glcm+DTC.xlsx'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# glcm + KNN Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test):\n\n        knn = KNeighborsClassifier(n_neighbors=5)\n        knn.fit(train_feature_glcm, y_train)\n        test_pred = knn.predict(test_feature_glcm)\n\n        accuracy = accuracy_score(y_test, test_pred)\n        recall = recall_score(y_test, test_pred, average='weighted')\n        precision = precision_score(y_test, test_pred, average='weighted')\n        f1 = f1_score(y_test, test_pred, average='weighted')\n        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'glcm+KNN.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:53:27.968307Z","iopub.execute_input":"2024-07-10T08:53:27.968680Z","iopub.status.idle":"2024-07-10T08:53:30.538561Z","shell.execute_reply.started":"2024-07-10T08:53:27.968648Z","shell.execute_reply":"2024-07-10T08:53:30.537269Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\nRun 2 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\nRun 3 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\nRun 4 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\nRun 5 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\nRun 6 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\nRun 7 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\nRun 8 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\nRun 9 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\nRun 10 - Accuracy: 0.86875000, Recall: 0.86875000, Precision: 0.87781146, F1 Score: 0.86843222, AUC: 0.91312500\n\nAverage Metrics:\nAccuracy: 0.86875000 (std: 0.00000000)\nRecall: 0.86875000 (std: 0.00000000)\nPrecision: 0.87781146 (std: 0.00000000)\nF1: 0.86843222 (std: 0.00000000)\nAuc: 0.91312500 (std: 0.00000000)\n\nMetrics results saved to 'glcm+KNN.xlsx'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# glcm + SVM Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\ndef train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test):\n\n        y_train_cat = convert_one_hot_to_categorical(y_train)\n        y_test_cat = convert_one_hot_to_categorical(y_test)\n\n        # Define and train SVM Classifier\n        svm = SVC(probability=True)\n        svm.fit(train_feature_glcm, y_train_cat)\n        test_pred = svm.predict(test_feature_glcm)\n        test_pred_proba = svm.predict_proba(test_feature_glcm)\n\n        # Calculate evaluation metrics\n        accuracy = accuracy_score(y_test_cat, test_pred)\n        recall = recall_score(y_test_cat, test_pred, average='weighted')\n        precision = precision_score(y_test_cat, test_pred, average='weighted')\n        f1 = f1_score(y_test_cat, test_pred, average='weighted')\n        auc = roc_auc_score(y_test_cat, test_pred_proba, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'glcm+SVM.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:53:30.540062Z","iopub.execute_input":"2024-07-10T08:53:30.540489Z","iopub.status.idle":"2024-07-10T08:54:44.864999Z","shell.execute_reply.started":"2024-07-10T08:53:30.540454Z","shell.execute_reply":"2024-07-10T08:54:44.863737Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79858125\nRun 2 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79900000\nRun 3 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79870208\nRun 4 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79838958\nRun 5 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79857500\nRun 6 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79864375\nRun 7 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79871458\nRun 8 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79891875\nRun 9 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79845417\nRun 10 - Accuracy: 0.52625000, Recall: 0.52625000, Precision: 0.56299196, F1 Score: 0.51652173, AUC: 0.79878125\n\nAverage Metrics:\nAccuracy: 0.52625000 (std: 0.00000000)\nRecall: 0.52625000 (std: 0.00000000)\nPrecision: 0.56299196 (std: 0.00000000)\nF1: 0.51652173 (std: 0.00000000)\nAuc: 0.79867604 (std: 0.00018128)\n\nMetrics results saved to 'glcm+SVM.xlsx'\n","output_type":"stream"}]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test):\n\n        y_train_cat = convert_one_hot_to_categorical(y_train)\n        y_test_cat = convert_one_hot_to_categorical(y_test)\n\n        xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n        xgb.fit(train_feature_glcm, y_train_cat)\n        test_pred = xgb.predict(test_feature_glcm)\n        test_pred_proba = xgb.predict_proba(test_feature_glcm)\n\n        # Calculate evaluation metrics\n        accuracy = accuracy_score(y_test_cat, test_pred)\n        recall = recall_score(y_test_cat, test_pred, average='weighted')\n        precision = precision_score(y_test_cat, test_pred, average='weighted')\n        f1 = f1_score(y_test_cat, test_pred, average='weighted')\n        auc = roc_auc_score(y_test_cat, test_pred_proba, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_glcm, y_train, test_feature_glcm, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'glcm+XGB.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T08:54:44.866777Z","iopub.execute_input":"2024-07-10T08:54:44.867272Z","iopub.status.idle":"2024-07-10T08:54:53.574656Z","shell.execute_reply.started":"2024-07-10T08:54:44.867226Z","shell.execute_reply":"2024-07-10T08:54:53.573372Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\nRun 2 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\nRun 3 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\nRun 4 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\nRun 5 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\nRun 6 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\nRun 7 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\nRun 8 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\nRun 9 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\nRun 10 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 1.00000000\n\nAverage Metrics:\nAccuracy: 0.99875000 (std: 0.00000000)\nRecall: 0.99875000 (std: 0.00000000)\nPrecision: 0.99875622 (std: 0.00000000)\nF1: 0.99874999 (std: 0.00000000)\nAuc: 1.00000000 (std: 0.00000000)\n\nMetrics results saved to 'glcm+XGB.xlsx'\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}