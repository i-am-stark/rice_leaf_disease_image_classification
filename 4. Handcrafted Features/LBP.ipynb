{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import glob as gb\n",
    "import cv2\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering the folder: BrownSpot\n",
      "Number of images in the folder: 1400\n",
      "Entering the folder: Blast\n",
      "Number of images in the folder: 1240\n",
      "Entering the folder: BacterialBlight\n",
      "Number of images in the folder: 1384\n",
      "Entering the folder: Tungro\n",
      "Number of images in the folder: 1108\n",
      "Entering to the folder name: BrownSpot\n",
      "Number of images in the folder is 200\n",
      "Entering to the folder name: Blast\n",
      "Number of images in the folder is 200\n",
      "Entering to the folder name: BacterialBlight\n",
      "Number of images in the folder is 200\n",
      "Entering to the folder name: Tungro\n",
      "Number of images in the folder is 200\n",
      "(5132, 224, 224, 3)\n",
      "********************\n",
      "(5132,)\n",
      "********************\n",
      "(800, 224, 224, 3)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "trainpath = '/Users/amanmaurya/Desktop/Rice Leaf Disease Image Classification/dataset/train'\n",
    "testpath = '/Users/amanmaurya/Desktop/Rice Leaf Disease Image Classification/dataset/test'\n",
    "\n",
    "# Image Processing - Training Data\n",
    "new_size = 224\n",
    "train_images = []\n",
    "train_labels = []\n",
    "class_disease = {'BacterialBlight': 0, 'Blast': 1, 'BrownSpot': 2, 'Tungro': 3}\n",
    "\n",
    "for i in os.listdir(trainpath):\n",
    "    if i in class_disease:\n",
    "        print(\"Entering the folder:\", i)\n",
    "        files = gb.glob(pathname=str(trainpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(trainpath + '/' + i + '/*.JPG'))\n",
    "        print(\"Number of images in the folder:\", len(files))\n",
    "        for j in files:\n",
    "            image_raw = cv2.imread(j)\n",
    "            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n",
    "            resize_image = cv2.resize(image, (new_size, new_size))\n",
    "            train_images.append(list(resize_image))\n",
    "            train_labels.append(class_disease[i])\n",
    "\n",
    "# Image Processing - Testing Data\n",
    "new_size = 224\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for i in os.listdir(testpath):\n",
    "    if i in class_disease:\n",
    "        print(\"Entering to the folder name:\", i)\n",
    "        files = gb.glob(pathname=str(testpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(testpath + '/' + i + '/*.JPG'))\n",
    "        print(\"Number of images in the folder is\", len(files))\n",
    "        for j in files:\n",
    "            image_raw = cv2.imread(j)\n",
    "            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n",
    "            resize_image = cv2.resize(image, (new_size, new_size))\n",
    "            test_images.append(list(resize_image))\n",
    "            test_labels.append(class_disease[i])\n",
    "\n",
    "def list_to_array_train(train_images, train_labels):\n",
    "    return np.array(train_images), np.array(train_labels)\n",
    "\n",
    "X_train, y_train = list_to_array_train(train_images, train_labels)\n",
    "\n",
    "def list_to_array_test(test_images, test_labels):\n",
    "    return np.array(test_images), np.array(test_labels)\n",
    "\n",
    "X_test, y_test = list_to_array_test(test_images, test_labels)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"*\" * 20)\n",
    "print(y_train.shape)\n",
    "print(\"*\" * 20)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "def keras_to_categorical(y_train, y_test):\n",
    "    return to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "y_train1 = y_train\n",
    "y_test1 = y_test\n",
    "y_train, y_test = keras_to_categorical(y_train, y_test)\n",
    "\n",
    "y_train1.shape, y_test1.shape\n",
    "\n",
    "def convert_one_hot_to_categorical(one_hot_labels):\n",
    "    return np.argmax(one_hot_labels, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Parameters for LBP\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "def compute_lbp_features(image):\n",
    "    if image.ndim == 3 and image.shape[2] == 3:  # Check if the image is RGB\n",
    "        gray_image = rgb2gray(image)\n",
    "    elif image.ndim == 2:  # If the image is already grayscale\n",
    "        gray_image = image\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "    \n",
    "    lbp = local_binary_pattern(gray_image, n_points, radius, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)  # Normalize the histogram\n",
    "    return hist\n",
    "\n",
    "def extract_lbp_features(X):\n",
    "    return np.array([compute_lbp_features(img) for img in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/skimage/feature/texture.py:360: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_feature_lbp = extract_lbp_features(X_train)\n",
    "test_feature_lbp = extract_lbp_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP + Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - Accuracy: 0.99750000, Recall: 0.99750000, Precision: 1.00000000, F1 Score: 0.99874687, AUC: 0.99875000\n",
      "Run 2 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 0.99875622, F1 Score: 0.99874999, AUC: 0.99916667\n",
      "Run 3 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 1.00000000, F1 Score: 0.99937343, AUC: 0.99937500\n",
      "Run 4 - Accuracy: 0.99625000, Recall: 0.99625000, Precision: 1.00000000, F1 Score: 0.99812030, AUC: 0.99812500\n",
      "Run 5 - Accuracy: 0.99750000, Recall: 0.99750000, Precision: 1.00000000, F1 Score: 0.99874687, AUC: 0.99875000\n",
      "Run 6 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 1.00000000, F1 Score: 0.99937343, AUC: 0.99937500\n",
      "Run 7 - Accuracy: 0.99750000, Recall: 0.99750000, Precision: 1.00000000, F1 Score: 0.99874687, AUC: 0.99875000\n",
      "Run 8 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 1.00000000, F1 Score: 0.99937343, AUC: 0.99937500\n",
      "Run 9 - Accuracy: 0.99875000, Recall: 0.99875000, Precision: 1.00000000, F1 Score: 0.99937343, AUC: 0.99937500\n",
      "Run 10 - Accuracy: 0.99750000, Recall: 0.99750000, Precision: 1.00000000, F1 Score: 0.99874687, AUC: 0.99875000\n",
      "\n",
      "Average Metrics:\n",
      "Accuracy: 0.99800000 (std: 0.00082916)\n",
      "Recall: 0.99800000 (std: 0.00082916)\n",
      "Precision: 0.99987562 (std: 0.00037313)\n",
      "F1: 0.99893515 (std: 0.00040105)\n",
      "Auc: 0.99897917 (std: 0.00040020)\n",
      "\n",
      "Metrics results saved to 'LBP+RF.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_fuse_and_evaluate_model(train_feature_lbp, y_train, test_feature_lbp, y_test):\n",
    "\n",
    "        rf = RandomForestClassifier()\n",
    "        rf = rf.fit(train_feature_lbp, y_train)\n",
    "        test_pred = rf.predict(test_feature_lbp)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, test_pred)\n",
    "        recall = recall_score(y_test, test_pred, average='weighted')\n",
    "        precision = precision_score(y_test, test_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n",
    "\n",
    "        return accuracy, recall, precision, f1, auc\n",
    "\n",
    "num_runs = 10\n",
    "results = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n",
    "\n",
    "for i in range(num_runs):\n",
    "    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_lbp, y_train, test_feature_lbp, y_test)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['precision'].append(precision)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(auc)\n",
    "    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "average_metrics = {metric: np.mean(values) for metric, values in results.items()}\n",
    "std_metrics = {metric: np.std(values) for metric, values in results.items()}\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'LBP+RF.xlsx'\n",
    "results_df.to_excel(output_file, index_label='Run')\n",
    "\n",
    "print(f\"\\nMetrics results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP + Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - Accuracy: 0.95625000, Recall: 0.95625000, Precision: 0.95695224, F1 Score: 0.95603714, AUC: 0.97083333\n",
      "Run 2 - Accuracy: 0.94000000, Recall: 0.94000000, Precision: 0.93994569, F1 Score: 0.93963669, AUC: 0.96000000\n",
      "Run 3 - Accuracy: 0.94875000, Recall: 0.94875000, Precision: 0.94866945, F1 Score: 0.94850790, AUC: 0.96583333\n",
      "Run 4 - Accuracy: 0.94625000, Recall: 0.94625000, Precision: 0.94657377, F1 Score: 0.94599240, AUC: 0.96416667\n",
      "Run 5 - Accuracy: 0.95500000, Recall: 0.95500000, Precision: 0.95502851, F1 Score: 0.95484358, AUC: 0.97000000\n",
      "Run 6 - Accuracy: 0.95500000, Recall: 0.95500000, Precision: 0.95490482, F1 Score: 0.95475411, AUC: 0.97000000\n",
      "Run 7 - Accuracy: 0.95250000, Recall: 0.95250000, Precision: 0.95256410, F1 Score: 0.95234094, AUC: 0.96833333\n",
      "Run 8 - Accuracy: 0.94750000, Recall: 0.94750000, Precision: 0.94740468, F1 Score: 0.94725913, AUC: 0.96500000\n",
      "Run 9 - Accuracy: 0.94375000, Recall: 0.94375000, Precision: 0.94378119, F1 Score: 0.94351303, AUC: 0.96250000\n",
      "Run 10 - Accuracy: 0.94625000, Recall: 0.94625000, Precision: 0.94652054, F1 Score: 0.94612253, AUC: 0.96416667\n",
      "\n",
      "Average Metrics:\n",
      "Accuracy: 0.94912500 (std: 0.00512500)\n",
      "Recall: 0.94912500 (std: 0.00512500)\n",
      "Precision: 0.94923450 (std: 0.00520102)\n",
      "F1: 0.94890074 (std: 0.00515964)\n",
      "Auc: 0.96608333 (std: 0.00341667)\n",
      "\n",
      "Metrics results saved to 'LBP+DTC.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_fuse_and_evaluate_model(train_feature_lbp, y_train, test_feature_lbp, y_test):\n",
    "\n",
    "        dt = DecisionTreeClassifier()\n",
    "        dt.fit(train_feature_lbp, y_train)\n",
    "        test_pred = dt.predict(test_feature_lbp)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, test_pred)\n",
    "        recall = recall_score(y_test, test_pred, average='weighted')\n",
    "        precision = precision_score(y_test, test_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n",
    "\n",
    "        return accuracy, recall, precision, f1, auc\n",
    "\n",
    "num_runs = 10\n",
    "results = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n",
    "\n",
    "for i in range(num_runs):\n",
    "    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_lbp, y_train, test_feature_lbp, y_test)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['precision'].append(precision)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(auc)\n",
    "    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "average_metrics = {metric: np.mean(values) for metric, values in results.items()}\n",
    "std_metrics = {metric: np.std(values) for metric, values in results.items()}\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'LBP+DTC.xlsx'\n",
    "results_df.to_excel(output_file, index_label='Run')\n",
    "\n",
    "print(f\"\\nMetrics results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP + KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "Run 2 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "Run 3 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "Run 4 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "Run 5 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "Run 6 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "Run 7 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "Run 8 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "Run 9 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "Run 10 - Accuracy: 0.97750000, Recall: 0.97750000, Precision: 0.98023770, F1 Score: 0.97808823, AUC: 0.98520833\n",
      "\n",
      "Average Metrics:\n",
      "Accuracy: 0.97750000 (std: 0.00000000)\n",
      "Recall: 0.97750000 (std: 0.00000000)\n",
      "Precision: 0.98023770 (std: 0.00000000)\n",
      "F1: 0.97808823 (std: 0.00000000)\n",
      "Auc: 0.98520833 (std: 0.00000000)\n",
      "\n",
      "Metrics results saved to 'LBP+KNN.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train_fuse_and_evaluate_model(train_feature_lbp, y_train, test_feature_lbp, y_test):\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(train_feature_lbp, y_train)\n",
    "        test_pred = knn.predict(test_feature_lbp)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, test_pred)\n",
    "        recall = recall_score(y_test, test_pred, average='weighted')\n",
    "        precision = precision_score(y_test, test_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n",
    "\n",
    "        return accuracy, recall, precision, f1, auc\n",
    "\n",
    "num_runs = 10\n",
    "results = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n",
    "\n",
    "for i in range(num_runs):\n",
    "    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_lbp, y_train, test_feature_lbp, y_test)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['precision'].append(precision)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(auc)\n",
    "    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "average_metrics = {metric: np.mean(values) for metric, values in results.items()}\n",
    "std_metrics = {metric: np.std(values) for metric, values in results.items()}\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'LBP+KNN.xlsx'\n",
    "results_df.to_excel(output_file, index_label='Run')\n",
    "\n",
    "print(f\"\\nMetrics results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP + SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84513125\n",
      "Run 2 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84516667\n",
      "Run 3 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84510208\n",
      "Run 4 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84517292\n",
      "Run 5 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84528542\n",
      "Run 6 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84513750\n",
      "Run 7 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84529375\n",
      "Run 8 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84526458\n",
      "Run 9 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84513958\n",
      "Run 10 - Accuracy: 0.64500000, Recall: 0.64500000, Precision: 0.66581334, F1 Score: 0.65082617, AUC: 0.84522708\n",
      "\n",
      "Average Metrics:\n",
      "Accuracy: 0.64500000 (std: 0.00000000)\n",
      "Recall: 0.64500000 (std: 0.00000000)\n",
      "Precision: 0.66581334 (std: 0.00000000)\n",
      "F1: 0.65082617 (std: 0.00000000)\n",
      "Auc: 0.84519208 (std: 0.00006639)\n",
      "\n",
      "Metrics results saved to 'LBP+SVM.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def train_fuse_and_evaluate_model(train_feature_lbp, y_train, test_feature_lbp, y_test):\n",
    "\n",
    "        y_train_cat = convert_one_hot_to_categorical(y_train)\n",
    "        y_test_cat = convert_one_hot_to_categorical(y_test)\n",
    "\n",
    "        # Define and train SVM Classifier\n",
    "        svm = SVC(probability=True)\n",
    "        svm.fit(train_feature_lbp, y_train_cat)\n",
    "        test_pred = svm.predict(test_feature_lbp)\n",
    "        test_pred_proba = svm.predict_proba(test_feature_lbp)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test_cat, test_pred)\n",
    "        recall = recall_score(y_test_cat, test_pred, average='weighted')\n",
    "        precision = precision_score(y_test_cat, test_pred, average='weighted')\n",
    "        f1 = f1_score(y_test_cat, test_pred, average='weighted')\n",
    "        auc = roc_auc_score(y_test_cat, test_pred_proba, multi_class='ovr', average='weighted')\n",
    "\n",
    "        return accuracy, recall, precision, f1, auc\n",
    "\n",
    "num_runs = 10\n",
    "results = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n",
    "\n",
    "for i in range(num_runs):\n",
    "    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_lbp, y_train, test_feature_lbp, y_test)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['precision'].append(precision)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(auc)\n",
    "    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "average_metrics = {metric: np.mean(values) for metric, values in results.items()}\n",
    "std_metrics = {metric: np.std(values) for metric, values in results.items()}\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = 'LBP+SVM.xlsx'\n",
    "results_df.to_excel(output_file, index_label='Run')\n",
    "\n",
    "print(f\"\\nMetrics results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
