{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8841239,"sourceType":"datasetVersion","datasetId":5321027}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pathlib\nimport os\nimport glob as gb\nimport cv2\nimport PIL\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow import keras\nfrom keras.models import Model","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:03:54.443054Z","iopub.execute_input":"2024-07-10T13:03:54.443445Z","iopub.status.idle":"2024-07-10T13:04:11.858094Z","shell.execute_reply.started":"2024-07-10T13:03:54.443411Z","shell.execute_reply":"2024-07-10T13:04:11.856745Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-10 13:03:59.472869: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-10 13:03:59.472974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-10 13:03:59.617949: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define paths\ntrainpath = '/kaggle/input/riceleafdataset/dataset/train'\ntestpath = '/kaggle/input/riceleafdataset/dataset/test'\n\n# Image Processing - Training Data\nnew_size = 224\ntrain_images = []\ntrain_labels = []\nclass_disease = {'BacterialBlight': 0, 'Blast': 1, 'BrownSpot': 2, 'Tungro': 3}\n\nfor i in os.listdir(trainpath):\n    if i in class_disease:\n        print(\"Entering the folder:\", i)\n        files = gb.glob(pathname=str(trainpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(trainpath + '/' + i + '/*.JPG'))\n        print(\"Number of images in the folder:\", len(files))\n        for j in files:\n            image_raw = cv2.imread(j)\n            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n            resize_image = cv2.resize(image, (new_size, new_size))\n            train_images.append(list(resize_image))\n            train_labels.append(class_disease[i])\n\n# Image Processing - Testing Data\nnew_size = 224\ntest_images = []\ntest_labels = []\n\nfor i in os.listdir(testpath):\n    if i in class_disease:\n        print(\"Entering to the folder name:\", i)\n        files = gb.glob(pathname=str(testpath + '/' + i + '/*.jpg')) + gb.glob(pathname=str(testpath + '/' + i + '/*.JPG'))\n        print(\"Number of images in the folder is\", len(files))\n        for j in files:\n            image_raw = cv2.imread(j)\n            image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n            resize_image = cv2.resize(image, (new_size, new_size))\n            test_images.append(list(resize_image))\n            test_labels.append(class_disease[i])\n\ndef list_to_array_train(train_images, train_labels):\n    return np.array(train_images), np.array(train_labels)\n\nX_train, y_train = list_to_array_train(train_images, train_labels)\n\ndef list_to_array_test(test_images, test_labels):\n    return np.array(test_images), np.array(test_labels)\n\nX_test, y_test = list_to_array_test(test_images, test_labels)\n\nprint(X_train.shape)\nprint(\"*\" * 20)\nprint(y_train.shape)\nprint(\"*\" * 20)\nprint(X_test.shape)\nprint(y_test.shape)\n\ndef keras_to_categorical(y_train, y_test):\n    return to_categorical(y_train), to_categorical(y_test)\n\ny_train1 = y_train\ny_test1 = y_test\ny_train, y_test = keras_to_categorical(y_train, y_test)\n\ny_train1.shape, y_test1.shape\n\ndef convert_one_hot_to_categorical(one_hot_labels):\n    return np.argmax(one_hot_labels, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:04:11.860409Z","iopub.execute_input":"2024-07-10T13:04:11.861080Z","iopub.status.idle":"2024-07-10T13:05:24.219451Z","shell.execute_reply.started":"2024-07-10T13:04:11.861043Z","shell.execute_reply":"2024-07-10T13:05:24.218097Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Entering the folder: Tungro\nNumber of images in the folder: 1108\nEntering the folder: BacterialBlight\nNumber of images in the folder: 1384\nEntering the folder: Blast\nNumber of images in the folder: 1240\nEntering the folder: BrownSpot\nNumber of images in the folder: 1400\nEntering to the folder name: Tungro\nNumber of images in the folder is 200\nEntering to the folder name: BacterialBlight\nNumber of images in the folder is 200\nEntering to the folder name: Blast\nNumber of images in the folder is 200\nEntering to the folder name: BrownSpot\nNumber of images in the folder is 200\n(5132, 224, 224, 3)\n********************\n(5132,)\n********************\n(800, 224, 224, 3)\n(800,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from skimage.feature import hog\nfrom tqdm import tqdm \n\ndef extract_hog_features(images):\n    hog_features = []\n    for img in tqdm(images, desc=\"Extracting HOG features\"):\n        if len(img.shape) == 3:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        feature, _ = hog(img, orientations=9, pixels_per_cell=(8, 8),\n                        cells_per_block=(2, 2), block_norm='L2-Hys', visualize=True)\n        hog_features.append(feature)\n    return np.array(hog_features)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:05:24.221123Z","iopub.execute_input":"2024-07-10T13:05:24.221976Z","iopub.status.idle":"2024-07-10T13:05:24.298004Z","shell.execute_reply.started":"2024-07-10T13:05:24.221930Z","shell.execute_reply":"2024-07-10T13:05:24.296632Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_feature_hog = extract_hog_features(X_train)\ntest_feature_hog = extract_hog_features(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:05:24.299276Z","iopub.execute_input":"2024-07-10T13:05:24.300039Z","iopub.status.idle":"2024-07-10T13:39:02.797581Z","shell.execute_reply.started":"2024-07-10T13:05:24.300001Z","shell.execute_reply":"2024-07-10T13:39:02.796014Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Extracting HOG features: 100%|██████████| 5132/5132 [29:07<00:00,  2.94it/s]\nExtracting HOG features: 100%|██████████| 800/800 [04:29<00:00,  2.96it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# hog + Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test):\n\n        rf = RandomForestClassifier()\n        rf = rf.fit(train_feature_hog, y_train)\n        test_pred = rf.predict(test_feature_hog)\n\n        accuracy = accuracy_score(y_test, test_pred)\n        recall = recall_score(y_test, test_pred, average='weighted')\n        precision = precision_score(y_test, test_pred, average='weighted')\n        f1 = f1_score(y_test, test_pred, average='weighted')\n        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'hog+RF.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T09:00:16.265526Z","iopub.execute_input":"2024-07-10T09:00:16.265858Z","iopub.status.idle":"2024-07-10T09:16:40.397041Z","shell.execute_reply.started":"2024-07-10T09:00:16.265830Z","shell.execute_reply":"2024-07-10T09:16:40.395819Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.43125000, Recall: 0.43125000, Precision: 0.99764151, F1 Score: 0.58945616, AUC: 0.71541667\nRun 2 - Accuracy: 0.42750000, Recall: 0.42750000, Precision: 1.00000000, F1 Score: 0.58800471, AUC: 0.71375000\nRun 3 - Accuracy: 0.41750000, Recall: 0.41750000, Precision: 1.00000000, F1 Score: 0.57875971, AUC: 0.70875000\nRun 4 - Accuracy: 0.42500000, Recall: 0.42500000, Precision: 1.00000000, F1 Score: 0.58887991, AUC: 0.71250000\nRun 5 - Accuracy: 0.43000000, Recall: 0.43000000, Precision: 1.00000000, F1 Score: 0.59240396, AUC: 0.71500000\nRun 6 - Accuracy: 0.43500000, Recall: 0.43500000, Precision: 1.00000000, F1 Score: 0.60131883, AUC: 0.71750000\nRun 7 - Accuracy: 0.43750000, Recall: 0.43750000, Precision: 1.00000000, F1 Score: 0.60019788, AUC: 0.71875000\nRun 8 - Accuracy: 0.42500000, Recall: 0.42500000, Precision: 1.00000000, F1 Score: 0.58802174, AUC: 0.71250000\nRun 9 - Accuracy: 0.44375000, Recall: 0.44375000, Precision: 1.00000000, F1 Score: 0.60629683, AUC: 0.72187500\nRun 10 - Accuracy: 0.43125000, Recall: 0.43125000, Precision: 1.00000000, F1 Score: 0.59371963, AUC: 0.71562500\n\nAverage Metrics:\nAccuracy: 0.43037500 (std: 0.00696083)\nRecall: 0.43037500 (std: 0.00696083)\nPrecision: 0.99976415 (std: 0.00070755)\nF1: 0.59270594 (std: 0.00761500)\nAuc: 0.71516667 (std: 0.00347836)\n\nMetrics results saved to 'hog+RF.xlsx'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# hog + Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test):\n\n        dt = DecisionTreeClassifier()\n        dt.fit(train_feature_hog, y_train)\n        test_pred = dt.predict(test_feature_hog)\n\n        accuracy = accuracy_score(y_test, test_pred)\n        recall = recall_score(y_test, test_pred, average='weighted')\n        precision = precision_score(y_test, test_pred, average='weighted')\n        f1 = f1_score(y_test, test_pred, average='weighted')\n        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'hog+DTC.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T09:16:40.398456Z","iopub.execute_input":"2024-07-10T09:16:40.399083Z","iopub.status.idle":"2024-07-10T09:55:28.203355Z","shell.execute_reply.started":"2024-07-10T09:16:40.399051Z","shell.execute_reply":"2024-07-10T09:55:28.202206Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.57750000, Recall: 0.57750000, Precision: 0.57791792, F1 Score: 0.57604971, AUC: 0.71833333\nRun 2 - Accuracy: 0.57250000, Recall: 0.57250000, Precision: 0.57273130, F1 Score: 0.57100130, AUC: 0.71500000\nRun 3 - Accuracy: 0.59125000, Recall: 0.59125000, Precision: 0.59017494, F1 Score: 0.58844049, AUC: 0.72750000\nRun 4 - Accuracy: 0.58875000, Recall: 0.58875000, Precision: 0.59025347, F1 Score: 0.58878016, AUC: 0.72583333\nRun 5 - Accuracy: 0.58000000, Recall: 0.58000000, Precision: 0.58085680, F1 Score: 0.57944598, AUC: 0.72000000\nRun 6 - Accuracy: 0.59250000, Recall: 0.59250000, Precision: 0.59290847, F1 Score: 0.59199801, AUC: 0.72833333\nRun 7 - Accuracy: 0.59875000, Recall: 0.59875000, Precision: 0.59918201, F1 Score: 0.59818726, AUC: 0.73250000\nRun 8 - Accuracy: 0.57875000, Recall: 0.57875000, Precision: 0.57767997, F1 Score: 0.57736797, AUC: 0.71916667\nRun 9 - Accuracy: 0.60125000, Recall: 0.60125000, Precision: 0.60106435, F1 Score: 0.60096641, AUC: 0.73416667\nRun 10 - Accuracy: 0.57375000, Recall: 0.57375000, Precision: 0.57195213, F1 Score: 0.57192055, AUC: 0.71583333\n\nAverage Metrics:\nAccuracy: 0.58550000 (std: 0.00981389)\nRecall: 0.58550000 (std: 0.00981389)\nPrecision: 0.58547214 (std: 0.01007750)\nF1: 0.58441578 (std: 0.01018211)\nAuc: 0.72366667 (std: 0.00654260)\n\nMetrics results saved to 'hog+DTC.xlsx'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# hog + KNN Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test):\n\n        knn = KNeighborsClassifier(n_neighbors=5)\n        knn.fit(train_feature_hog, y_train)\n        test_pred = knn.predict(test_feature_hog)\n\n        accuracy = accuracy_score(y_test, test_pred)\n        recall = recall_score(y_test, test_pred, average='weighted')\n        precision = precision_score(y_test, test_pred, average='weighted')\n        f1 = f1_score(y_test, test_pred, average='weighted')\n        auc = roc_auc_score(y_test, test_pred, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'hog+KNN.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T09:55:28.204876Z","iopub.execute_input":"2024-07-10T09:55:28.205261Z","iopub.status.idle":"2024-07-10T09:56:11.881000Z","shell.execute_reply.started":"2024-07-10T09:55:28.205229Z","shell.execute_reply":"2024-07-10T09:56:11.879927Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\nRun 2 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\nRun 3 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\nRun 4 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\nRun 5 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\nRun 6 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\nRun 7 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\nRun 8 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\nRun 9 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\nRun 10 - Accuracy: 0.63750000, Recall: 0.63750000, Precision: 0.73384469, F1 Score: 0.63640990, AUC: 0.76541667\n\nAverage Metrics:\nAccuracy: 0.63750000 (std: 0.00000000)\nRecall: 0.63750000 (std: 0.00000000)\nPrecision: 0.73384469 (std: 0.00000000)\nF1: 0.63640990 (std: 0.00000000)\nAuc: 0.76541667 (std: 0.00000000)\n\nMetrics results saved to 'hog+KNN.xlsx'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# hog + SVM Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\ndef train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test):\n\n        y_train_cat = convert_one_hot_to_categorical(y_train)\n        y_test_cat = convert_one_hot_to_categorical(y_test)\n\n        # Define and train SVM Classifier\n        svm = SVC(probability=True)\n        svm.fit(train_feature_hog, y_train_cat)\n        test_pred = svm.predict(test_feature_hog)\n        test_pred_proba = svm.predict_proba(test_feature_hog)\n\n        # Calculate evaluation metrics\n        accuracy = accuracy_score(y_test_cat, test_pred)\n        recall = recall_score(y_test_cat, test_pred, average='weighted')\n        precision = precision_score(y_test_cat, test_pred, average='weighted')\n        f1 = f1_score(y_test_cat, test_pred, average='weighted')\n        auc = roc_auc_score(y_test_cat, test_pred_proba, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'hog+SVM.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T09:56:11.883013Z","iopub.execute_input":"2024-07-10T09:56:11.883385Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.89125000, Recall: 0.89125000, Precision: 0.89220896, F1 Score: 0.88960075, AUC: 0.98907917\nRun 2 - Accuracy: 0.89125000, Recall: 0.89125000, Precision: 0.89220896, F1 Score: 0.88960075, AUC: 0.98900417\n","output_type":"stream"}]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\ndef train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test):\n\n        y_train_cat = convert_one_hot_to_categorical(y_train)\n        y_test_cat = convert_one_hot_to_categorical(y_test)\n\n        xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n        xgb.fit(train_feature_hog, y_train_cat)\n        test_pred = xgb.predict(test_feature_hog)\n        test_pred_proba = xgb.predict_proba(test_feature_hog)\n\n        # Calculate evaluation metrics\n        accuracy = accuracy_score(y_test_cat, test_pred)\n        recall = recall_score(y_test_cat, test_pred, average='weighted')\n        precision = precision_score(y_test_cat, test_pred, average='weighted')\n        f1 = f1_score(y_test_cat, test_pred, average='weighted')\n        auc = roc_auc_score(y_test_cat, test_pred_proba, multi_class='ovr', average='weighted')\n\n        return accuracy, recall, precision, f1, auc\n\nnum_runs = 10\nresults = {'accuracy': [], 'recall': [], 'precision': [], 'f1': [], 'auc': []}\n\nfor i in range(num_runs):\n    accuracy, recall, precision, f1, auc = train_fuse_and_evaluate_model(train_feature_hog, y_train, test_feature_hog, y_test)\n    results['accuracy'].append(accuracy)\n    results['recall'].append(recall)\n    results['precision'].append(precision)\n    results['f1'].append(f1)\n    results['auc'].append(auc)\n    print(f\"Run {i+1} - Accuracy: {accuracy:.8f}, Recall: {recall:.8f}, Precision: {precision:.8f}, F1 Score: {f1:.8f}, AUC: {auc:.8f}\")\n\n# Calculate average metrics\naverage_metrics = {metric: np.mean(values) for metric, values in results.items()}\nstd_metrics = {metric: np.std(values) for metric, values in results.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics:\")\nfor metric, value in average_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.8f} (std: {std_metrics[metric]:.8f})\")\n\n# Convert results to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to an Excel file\noutput_file = 'hog+XGB.xlsx'\nresults_df.to_excel(output_file, index_label='Run')\n\nprint(f\"\\nMetrics results saved to '{output_file}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:39:02.800536Z","iopub.execute_input":"2024-07-10T13:39:02.801354Z","iopub.status.idle":"2024-07-10T17:39:36.808646Z","shell.execute_reply.started":"2024-07-10T13:39:02.801299Z","shell.execute_reply":"2024-07-10T17:39:36.807091Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Run 1 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\nRun 2 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\nRun 3 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\nRun 4 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\nRun 5 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\nRun 6 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\nRun 7 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\nRun 8 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\nRun 9 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\nRun 10 - Accuracy: 0.86625000, Recall: 0.86625000, Precision: 0.87183054, F1 Score: 0.86231055, AUC: 0.98116875\n\nAverage Metrics:\nAccuracy: 0.86625000 (std: 0.00000000)\nRecall: 0.86625000 (std: 0.00000000)\nPrecision: 0.87183054 (std: 0.00000000)\nF1: 0.86231055 (std: 0.00000000)\nAuc: 0.98116875 (std: 0.00000000)\n\nMetrics results saved to 'hog+XGB.xlsx'\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}